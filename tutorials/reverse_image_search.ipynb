{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "assigned-minneapolis",
   "metadata": {},
   "source": [
    "# Reverse Image Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-creature",
   "metadata": {},
   "source": [
    "## Scenario introduction\n",
    "\n",
    "**Reverse image search** helps you search for similar or related images given an input image. Reverse image search is a [content-based image retrieval](https://en.wikipedia.org/wiki/Content-based_image_retrieval) (CBIR) query technique that involves providing the CBIR system with a query image that it will then base its search upon. Unlike the traditional image search (which often relies on performing text queries across user-generated labels), reverse image search is based on the content of the image itself.\n",
    "\n",
    "A few applications of reverse image search include finding the original source of an image, searching for similar content, and product recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-secret",
   "metadata": {},
   "source": [
    "## Tutorial overview\n",
    "\n",
    "Building a reverse image search system typically involves the following steps:\n",
    "1. model and pipeline selection\n",
    "2. computing embedding vectors for the existing image dataset\n",
    "3. insert all generated embedding vectors into a vector database\n",
    "4. process search queries\n",
    "\n",
    "A block diagram for a basic reverse image search system is shown in the images below. The first image shows how an existing image dataset is transformed into embedding vectors and inserted into a vector database, while the second image shows how the system processes query images.\n",
    "\n",
    "**FIRST IMAGE HERE(INSERT)**\n",
    "\n",
    "![img](https://github.com/towhee-io/towhee/raw/main/docs/tutorials/reverse_image_search_step1.png?raw=true)\n",
    "\n",
    "**SECOND IMAGE HERE(SEARCH)**\n",
    "\n",
    "![img](https://github.com/towhee-io/towhee/raw/main/docs/tutorials/reverse_image_search_step2.png?raw=true)\n",
    "\n",
    "In the upcoming sections, we will first walk you through some of the prep work required for this tutorial. After that, we will elaborate on each of the four steps mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-premises",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "In this step, we will download the image dataset, install [Towhee](https://towhee.io), and setup [Milvus](https://milvus.io), an open source vector database.\n",
    "\n",
    "### Download the image dataset\n",
    "\n",
    "In this tutorial, we will use a subset of the ImageNet dataset (100 classes, 10 images for each class). You can download the dataset via:\n",
    "\n",
    "- Google Drive: https://drive.google.com/file/d/1bg1RtUjeZlOfV2BiA2nf7sn5Jec9b-9I/view?usp=sharing\n",
    "- Dropbox: https://www.dropbox.com/s/ucv15cxblok84x0/image_dataset.zip?dl=0\n",
    "- Aliyun Drive: https://www.aliyundrive.com/s/dLdeDWEhcnr\n",
    "\n",
    "In this tutorial, we will use `gdown` to download and unzip the data from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install gdown\n",
    "! gdown \"https://drive.google.com/uc?id=1bg1RtUjeZlOfV2BiA2nf7sn5Jec9b-9I\"\n",
    "! unzip -q image_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-scoop",
   "metadata": {},
   "source": [
    "The downloaded data contains two directories - `dataset` for the image dataset and `query` for the query images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-sample",
   "metadata": {},
   "source": [
    "### Install Towhee\n",
    "\n",
    "We'll use `pip` in this tutorial. We also support installing Towhee via `conda` as well as from source; check out [this page](https://docs.towhee.io/get-started/install) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install towhee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-secretary",
   "metadata": {},
   "source": [
    "### Setup Milvus\n",
    "\n",
    "Milvus is an open-source vector database built to power embedding similarity search and AI applications. More info about Milvus is available [here](https://github.com/milvus-io/milvus).\n",
    "\n",
    "We'll be using `docker-compose` to install Milvus standalone. Before installing Milvus (see the [official Milvus installation guide](https://milvus.io/docs/v2.0.0/install_standalone-docker.md)), make sure you have the necessary [prerequisites](https://milvus.io/docs/v2.0.0/prerequisite-docker.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the latest docker-compose file\n",
    "! wget https://github.com/milvus-io/milvus/releases/download/v2.0.0-pre-ga/milvus-standalone-docker-compose.yml -O docker-compose.yml\n",
    "# start the Milvus service\n",
    "! docker-compose up -d\n",
    "# check the state of the containers\n",
    "! docker-compose ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-springer",
   "metadata": {},
   "source": [
    "We will also need to install Python bindings for Milvus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install pymilvus==2.0.0rc9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-pleasure",
   "metadata": {},
   "source": [
    "## Model and pipeline selection\n",
    "\n",
    "The first step in building a reverse image search system is selecting an appropriate embedding model and one of its associated pipelines. Within Towhee, all pipelines can be found on the [Towhee hub](https://towhee.io/pipelines). Clicking on any of the categories on the right hand side of the page will filter the results based on the specified task; selecting the `image-embedding` category will reveal all image embedding pipelines that Towhee offers. We also provide a summary of popular image embedding pipelines [here](https://docs.towhee.io/pipelines/image-embedding). \n",
    "\n",
    "Resource requirements, accuracy, inference latency are key trade-offs when selecting a proper pipeline. Towhee provides a multitude of pipelines to meet various application demands. The current state-of-the-art embedding pipelines are ensemble pipelines that include multiple models (our [best ensemble](https://towhee.io/towhee/image-embedding-3ways-ensemble-v1) combines the Swin Transformer with EfficientNet and Resnet-101). These pipelines are fairly computational expensive. In contrast, if a slightly less \"accurate\" but much faster pipeline is okay for your application, we recommend EfficientNet ([image-embedding-efficientnetb7](https://towhee.io/towhee/image-embedding-efficientnetb7)). For demonstration purposes, we will be using Resnet-50 ([image-embedding-resnet50](https://towhee.io/towhee/image-embedding-resnet50)) in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from towhee import pipeline\n",
    "embedding_pipeline = pipeline('towhee/image-embedding-resnet50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-christmas",
   "metadata": {},
   "source": [
    "## Computing embedding vectors for the existing image dataset\n",
    "\n",
    "With an optimal pipeline selected, computing embedding vectors over our image dataset is the next step. All `image-embedding` Towhee pipelines output an embedding vector given an image path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_path = './image_dataset/dataset/'\n",
    "images = []\n",
    "vectors = []\n",
    "\n",
    "for img_path in Path(dataset_path).glob('*'):\n",
    "    vec = embedding_pipeline(str(img_path))\n",
    "    norm_vec = vec / np.linalg.norm(vec)\n",
    "    vectors.append(norm_vec.tolist())\n",
    "    images.append(str(img_path.resolve()))\n",
    "\n",
    "len(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-prophet",
   "metadata": {},
   "source": [
    "After running it, you will get 1000 vectors(100 class * 10 images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-movie",
   "metadata": {},
   "source": [
    "## Insert all generated embedding vectors into a vector database\n",
    "\n",
    "While brute-force computation of distances between queries and all image dataset vectors is perfectly fine for small datasets, scaling to billions of image dataset items requires a production-grade vector database that utilizes a search index to greatly speed up the query process. Here, we'll insert the vectors computed in the previous section into a Milvus collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymilvus as milvus\n",
    "\n",
    "collection_name = 'reverse_image_search'\n",
    "vec_dim = len(vectors[0])\n",
    "\n",
    "# connect to local Milvus service\n",
    "milvus.connections.connect(host='127.0.0.1', port=19530)\n",
    "\n",
    "# create collection\n",
    "id_field = milvus.FieldSchema(name=\"id\", dtype=milvus.DataType.INT64, is_primary=True, auto_id=True)\n",
    "vec_field = milvus.FieldSchema(name=\"vec\", dtype=milvus.DataType.FLOAT_VECTOR, dim=vec_dim)\n",
    "schema = milvus.CollectionSchema(fields=[id_field, vec_field])\n",
    "collection = milvus.Collection(name=collection_name, schema=schema)\n",
    "\n",
    "# insert data to Milvus\n",
    "res = collection.insert([vectors])\n",
    "collection.load()\n",
    "img_dict = {}\n",
    "\n",
    "# maintain mappings between primary keys and the original images for image retrieval\n",
    "for i, key in enumerate(res.primary_keys):\n",
    "    img_dict[key] = images[i]\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-liberal",
   "metadata": {},
   "source": [
    "## Process search queries\n",
    "\n",
    "We can use the same pipeline to generate an embedding vector for each query image. We can then search across the collection using the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-lying",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_img_path = './image_dataset/query'\n",
    "query_images = []\n",
    "query_vectors = []\n",
    "top_k = 5\n",
    "\n",
    "for img_path in Path(query_img_path).glob('*'):\n",
    "    vec = embedding_pipeline(str(img_path))\n",
    "    norm_vec = vec / np.linalg.norm(vec)\n",
    "    query_vectors.append(norm_vec.tolist())\n",
    "    query_images.append(str(img_path.resolve()))\n",
    "\n",
    "query_results = collection.search(data=query_vectors, anns_field=\"vec\", param={\"metric_type\": 'L2'}, limit=top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-panel",
   "metadata": {},
   "source": [
    "Display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "for i in range(len(query_results)):\n",
    "    results = query_results[i]\n",
    "    query_file = query_images[i]\n",
    "    \n",
    "    result_files = [img_dict[result.id] for result in results]\n",
    "    distances = [result.distance for result in results]\n",
    "\n",
    "    fig_query, ax_query = plt.subplots(1,1, figsize=(5,5))\n",
    "    ax_query.imshow(Image.open(query_file))\n",
    "    ax_query.set_title(\"Searched Image\\n\")\n",
    "    ax_query.axis('off')\n",
    "\n",
    "    fig, ax = plt.subplots(1,len(result_files),figsize=(20,20))\n",
    "    for x in range(len(result_files)):\n",
    "        ax[x].imshow(Image.open(result_files[x]))\n",
    "        ax[x].set_title('dist: ' + str(distances[x])[0:5])\n",
    "        ax[x].axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
