{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52079653",
   "metadata": {},
   "source": [
    "# Music Recognition System\n",
    "\n",
    "\n",
    "## Scenario Introduction\n",
    "\n",
    "A **music recognition system** automatically identifies a piece of music by matching a short snippet against a database of known music. Compared to the traditional methods using frequency domain analysis, the use of embedding vectors generated by 1D convolutional neural networks improves recall and can, in some cases, improve query speed.\n",
    "\n",
    "A music recognition system generally transforms audio data to embeddings and compares similarity based on distances between embeddings. Therefore, an encoder converting audio to embedding and a database for vector storage and retrieval are main components.\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "Normally an audio embedding pipeline generates a set of embeddings given an audio path, which composes a unique fingerprint representing the input music. Each embedding corresponds to features extracted for a snippet of the input audio. By comparing embeddings of audio snippets, the system can determine the similarity between audios. The image below explains the music fingerprinting by audio embeddings.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/towhee-io/towhee/raw/main/docs/tutorials/music_embedding.png\" width=500/>\n",
    "</p>\n",
    "\n",
    "A block diagram for a basic music recognition system is shown in images below. The first image illustrates how the system transforms a music dataset to vectors with Towhee and then inserts all vectors into Milvus. The second image shows the querying process of an unknown music snippet.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/towhee-io/towhee/raw/main/docs/tutorials/music_recog_system.png\" width=500/>\n",
    "</p>\n",
    "\n",
    "Building a music recognition system typically involves the following steps:\n",
    "\n",
    "1. Model and pipeline selection\n",
    "2. Computing fingerprints for the existing music dataset\n",
    "3. Insert all generated vectors into a vector database\n",
    "4. Identify an unknown music snippet by similarity search of vectors\n",
    "\n",
    "In the upcoming sections, we will first walk you through some of the prep work for this tutorial. After that, we will elaborate on each of the four steps mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42adc31f",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "In this step, we will download the music dataset, install [Towhee](https://towhee.io), and setup [Milvus](https://milvus.io), an open source vector database.\n",
    "\n",
    "### Download dataset\n",
    "\n",
    "This tutorial uses a subset of [GTZAN](http://marsyas.info/downloads/datasets.html). It contains 10 tracks of music each 30 seconds long and 1 test clip of music 10 seconds long. You can download it via:\n",
    "\n",
    "- [Google Drive](https://drive.google.com/file/d/1gHF8HDzXxeSy8bhtaTVueRFVPdj6ZEZJ/view?usp=sharing)\n",
    "- [Dropbox](https://www.dropbox.com/s/hw5vgb385alubb3/music_dataset.zip?dl=0)\n",
    "- [Aliyundrive](https://www.aliyundrive.com/s/sjb3c48CHc1)\n",
    "\n",
    "Here we will use `gdown` to download and unzip the data from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5434c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install gdown\n",
    "! gdown 'https://drive.google.com/uc?id=1gHF8HDzXxeSy8bhtaTVueRFVPdj6ZEZJ'\n",
    "! unzip -q music_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e460c4",
   "metadata": {},
   "source": [
    "The folder `data` downloaded contains two directories - `music_dataset` for the music dataset and `query` for the music snippet to test querying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3624ec",
   "metadata": {},
   "source": [
    "### Install Towhee\n",
    "\n",
    "We'll use `pip` in this tutorial. We also support installing Towhee via `conda` as well as from source; check out [this page](https://docs.towhee.io/get-started/install) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83334694",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install towhee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a8aca",
   "metadata": {},
   "source": [
    "### Setup Milvus\n",
    "\n",
    "Milvus is an open-source vector database built to power embedding similarity search and AI applications. More info about Milvus is available [here](https://github.com/milvus-io/milvus).\n",
    "\n",
    "We'll be using `docker-compose` to install Milvus standalone. Before installing Milvus (see the [official Milvus installation guide](https://milvus.io/docs/v2.0.0/install_standalone-docker.md)), make sure you meet all [prerequisites](https://milvus.io/docs/v2.0.0/prerequisite-docker.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a7a3f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download the latest docker-compose file\n",
    "! wget https://github.com/milvus-io/milvus/releases/download/v2.0.0-pre-ga/milvus-standalone-docker-compose.yml -O docker-compose.yml\n",
    "# start the Milvus service\n",
    "! docker-compose up -d\n",
    "# check the state of the containers\n",
    "! docker-compose ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c50cac",
   "metadata": {},
   "source": [
    "We will also need to install Python SDK for Milvus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e73dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install pymilvus==2.0.0rc9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f9fc4",
   "metadata": {},
   "source": [
    "## Steps in Python\n",
    "\n",
    "### 1. Model and pipeline selection\n",
    "\n",
    "The first step in building a music recognition system is selecting an appropriate embedding model and one of its associated pipelines. Within Towhee, all pipelines can be found on the [Towhee hub](https://towhee.io/pipelines). Clicking on any of the categories on the right hand side of the page will filter the results based on the specified task; selecting the `audio-embedding` category will reveal all audio embedding pipelines that Towhee offers. We also provide a summary of popular audio embedding pipelines [here](https://docs.towhee.io/pipelines/audio-embedding). \n",
    "\n",
    "Resource requirements, accuracy, inference latency are key trade-offs when selecting a proper pipeline. Towhee provides a multitude of pipelines to meet various application demands. For demonstration purposes, we will be using VGGish ([audio-embedding-vggish](https://towhee.io/towhee/audio-embedding-vggish)) in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f12cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from towhee import pipeline\n",
    "embedding_pipeline = pipeline('towhee/audio-embedding-vggish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd2d55",
   "metadata": {},
   "source": [
    "### 2. Computing fingerprints for the existing music dataset\n",
    "\n",
    "With an optimal pipeline selected, computing music fingerprints over our music dataset is the next step. All `audio-embedding` Towhee pipelines output a set of vectors (a music fingerprint) given an audio path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd332c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_path = './music_dataset/dataset'\n",
    "music_list = [f for f in Path(dataset_path).glob('*')]\n",
    "vec_sets = []\n",
    "\n",
    "for audio_path in music_list:\n",
    "    vecs = embedding_pipeline(str(audio_path))\n",
    "    norm_vecs = [vec / np.linalg.norm(vec) for vec in vecs[0][0]]\n",
    "    vec_sets.append(norm_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3751c9",
   "metadata": {},
   "source": [
    "### 3. Insert all generated embedding vectors into a vector database\n",
    "\n",
    "While brute-force computation of distances between queries and all audio vectors is perfectly fine for small datasets, scaling to billions of music dataset items requires a production-grade vector database that utilizes a search index to greatly speed up the query process. Here, we'll insert vectors computed in the previous section into a Milvus collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f10e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymilvus as milvus\n",
    "\n",
    "# Gather vectors in a list\n",
    "vectors = []\n",
    "for i in range(len(vec_sets)):\n",
    "    for vec in vec_sets[i]:\n",
    "        vectors.append(vec)\n",
    "\n",
    "collection_name = 'music_recognition'\n",
    "vec_dim = len(vectors[0])\n",
    "\n",
    "# connect to local Milvus service\n",
    "milvus.connections.connect(host='localhost', port=19530)\n",
    "\n",
    "# create collection\n",
    "id_field = milvus.FieldSchema(name=\"id\", dtype=milvus.DataType.INT64, descrition=\"int64\", is_primary=True, auto_id=True)\n",
    "vec_field = milvus.FieldSchema(name=\"vec\", dtype=milvus.DataType.FLOAT_VECTOR, dim=vec_dim)\n",
    "schema = milvus.CollectionSchema(fields=[id_field, vec_field])\n",
    "collection = milvus.Collection(name=collection_name, schema=schema)\n",
    "\n",
    "# insert data to Milvus\n",
    "res = collection.insert([vectors])\n",
    "\n",
    "# maintain mappings between primary keys of music clips and the original music for retrieval\n",
    "full_music_list = []\n",
    "music_dict = dict()\n",
    "for i in range(len(vec_sets)):\n",
    "    for _ in range(len(vec_sets[i])):\n",
    "        full_music_list.append(music_list[i])        \n",
    "for i, pk in enumerate(res.primary_keys):\n",
    "    music_dict[pk] = full_music_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0102c",
   "metadata": {},
   "source": [
    "### 4. Identify an unknown music snippet by similarity search of vectors\n",
    "\n",
    "We can use the same pipeline to generate a set of vectors for a query audio. Then searching across the collection will find the most closest fingerprint piece for each vector in the set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ecfcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_audio_path = './music_dataset/query/blues_clip.wav'\n",
    "query_vecs = embedding_pipeline(query_audio_path) # Get vectors of the given audio\n",
    "norm_query_vecs = [vec / np.linalg.norm(vec) for vec in query_vecs[0][0]] # Normalize vectors\n",
    "\n",
    "collection.load()\n",
    "results = collection.search(data=norm_query_vecs, anns_field=\"vec\", param={\"metric_type\": 'L2'}, limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4fef9",
   "metadata": {},
   "source": [
    "#### Display result\n",
    "\n",
    "Vector ids returned by Milvus are then fed into the dictionary, which returns the corresponding paths of the closest-matching audio files in storage. These query results will vote for the final identification of the input audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c2ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "votes = [music_dict[x.ids[0]] for x in results]\n",
    "pred = max(set(votes), key = votes.count)\n",
    "\n",
    "print(str(pred))\n",
    "IPython.display.Audio(Path(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with original query snippet\n",
    "print(str(query_audio_path))\n",
    "IPython.display.Audio(Path(query_audio_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374afdf8",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "This is a simple tutorial of a basic music recognition system. More complex systems are required in production. To make your own solution, we suggest to optimize the system performance with following options:\n",
    "\n",
    "1. Preprocess data with de-noisy, sliding window method, etc.\n",
    "2. Switch Towhee pipelines (change model) based on your data\n",
    "3. Combine with other search strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a9d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:towhee]",
   "language": "python",
   "name": "conda-env-towhee-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
